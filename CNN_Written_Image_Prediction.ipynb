{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  \n",
    "import torch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='C:/Users/roboc/OneDrive/Desktop/New folder (2)/New folder', train=True,  download=True,transform=ToTensor())\n",
    "test_dataset = datasets.MNIST(root='C:/Users/roboc/OneDrive/Desktop/New folder (2)/New folder', train=False,  download=True,transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: C:/Users/roboc/OneDrive/Desktop/New folder (2)/New folder\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loaders = {\n",
    "    'train': DataLoader(train_dataset, \n",
    "                        batch_size=100, \n",
    "                        shuffle=True,\n",
    "                        num_workers=1),\n",
    "    'test': DataLoader(test_dataset, \n",
    "                       batch_size=100, \n",
    "                       shuffle=True,\n",
    "                       num_workers=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creating the neural network\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320,50)\n",
    "        self.fc2 =  nn.Linear(50,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x= F.relu(F.max_pool2d(self.conv1(x),2))\n",
    "        x= F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2))\n",
    "        x=x.view(-1,320)\n",
    "        x= F.relu(self.fc1(x))\n",
    "        x= F.dropout(x, training=self.training)\n",
    "        x=self.fc2(x)\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roboc\\AppData\\Local\\Temp\\ipykernel_6148\\4231561469.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303921\n",
      "Train Epoch: 1 [2500/60000 (4%)]\tLoss: 2.225593\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 2.103435\n",
      "Train Epoch: 1 [7500/60000 (12%)]\tLoss: 1.886424\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.843193\n",
      "Train Epoch: 1 [12500/60000 (21%)]\tLoss: 1.778751\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 1.772639\n",
      "Train Epoch: 1 [17500/60000 (29%)]\tLoss: 1.684545\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.698216\n",
      "Train Epoch: 1 [22500/60000 (38%)]\tLoss: 1.712685\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 1.710953\n",
      "Train Epoch: 1 [27500/60000 (46%)]\tLoss: 1.686043\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.692844\n",
      "Train Epoch: 1 [32500/60000 (54%)]\tLoss: 1.616654\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 1.601175\n",
      "Train Epoch: 1 [37500/60000 (62%)]\tLoss: 1.603380\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.632780\n",
      "Train Epoch: 1 [42500/60000 (71%)]\tLoss: 1.643232\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 1.637012\n",
      "Train Epoch: 1 [47500/60000 (79%)]\tLoss: 1.599263\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 1.613297\n",
      "Train Epoch: 1 [52500/60000 (88%)]\tLoss: 1.560624\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 1.567674\n",
      "Train Epoch: 1 [57500/60000 (96%)]\tLoss: 1.591967\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 9352/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.567745\n",
      "Train Epoch: 2 [2500/60000 (4%)]\tLoss: 1.615027\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 1.588624\n",
      "Train Epoch: 2 [7500/60000 (12%)]\tLoss: 1.600183\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1.632354\n",
      "Train Epoch: 2 [12500/60000 (21%)]\tLoss: 1.606540\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 1.602334\n",
      "Train Epoch: 2 [17500/60000 (29%)]\tLoss: 1.587703\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.617732\n",
      "Train Epoch: 2 [22500/60000 (38%)]\tLoss: 1.579757\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 1.592558\n",
      "Train Epoch: 2 [27500/60000 (46%)]\tLoss: 1.614061\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.593167\n",
      "Train Epoch: 2 [32500/60000 (54%)]\tLoss: 1.579788\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 1.559534\n",
      "Train Epoch: 2 [37500/60000 (62%)]\tLoss: 1.585903\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.539382\n",
      "Train Epoch: 2 [42500/60000 (71%)]\tLoss: 1.625015\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 1.571604\n",
      "Train Epoch: 2 [47500/60000 (79%)]\tLoss: 1.628190\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.535066\n",
      "Train Epoch: 2 [52500/60000 (88%)]\tLoss: 1.576300\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 1.598613\n",
      "Train Epoch: 2 [57500/60000 (96%)]\tLoss: 1.575837\n",
      "\n",
      "Test set: Average loss: 0.0151, Accuracy: 9495/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.637313\n",
      "Train Epoch: 3 [2500/60000 (4%)]\tLoss: 1.522381\n",
      "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 1.565333\n",
      "Train Epoch: 3 [7500/60000 (12%)]\tLoss: 1.576095\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.587410\n",
      "Train Epoch: 3 [12500/60000 (21%)]\tLoss: 1.610352\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 1.610621\n",
      "Train Epoch: 3 [17500/60000 (29%)]\tLoss: 1.611474\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.575332\n",
      "Train Epoch: 3 [22500/60000 (38%)]\tLoss: 1.553313\n",
      "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 1.570991\n",
      "Train Epoch: 3 [27500/60000 (46%)]\tLoss: 1.530925\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.605252\n",
      "Train Epoch: 3 [32500/60000 (54%)]\tLoss: 1.530374\n",
      "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 1.580116\n",
      "Train Epoch: 3 [37500/60000 (62%)]\tLoss: 1.562484\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.573916\n",
      "Train Epoch: 3 [42500/60000 (71%)]\tLoss: 1.562114\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 1.566141\n",
      "Train Epoch: 3 [47500/60000 (79%)]\tLoss: 1.541441\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.517165\n",
      "Train Epoch: 3 [52500/60000 (88%)]\tLoss: 1.533787\n",
      "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 1.563065\n",
      "Train Epoch: 3 [57500/60000 (96%)]\tLoss: 1.571118\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy: 9571/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.549406\n",
      "Train Epoch: 4 [2500/60000 (4%)]\tLoss: 1.573141\n",
      "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 1.532366\n",
      "Train Epoch: 4 [7500/60000 (12%)]\tLoss: 1.570002\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.540014\n",
      "Train Epoch: 4 [12500/60000 (21%)]\tLoss: 1.540017\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 1.544642\n",
      "Train Epoch: 4 [17500/60000 (29%)]\tLoss: 1.525305\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.602740\n",
      "Train Epoch: 4 [22500/60000 (38%)]\tLoss: 1.514295\n",
      "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 1.521595\n",
      "Train Epoch: 4 [27500/60000 (46%)]\tLoss: 1.540273\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.533950\n",
      "Train Epoch: 4 [32500/60000 (54%)]\tLoss: 1.513923\n",
      "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 1.581821\n",
      "Train Epoch: 4 [37500/60000 (62%)]\tLoss: 1.531084\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.610269\n",
      "Train Epoch: 4 [42500/60000 (71%)]\tLoss: 1.547809\n",
      "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 1.542103\n",
      "Train Epoch: 4 [47500/60000 (79%)]\tLoss: 1.534863\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.566457\n",
      "Train Epoch: 4 [52500/60000 (88%)]\tLoss: 1.563568\n",
      "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 1.546134\n",
      "Train Epoch: 4 [57500/60000 (96%)]\tLoss: 1.588829\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy: 9601/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.569876\n",
      "Train Epoch: 5 [2500/60000 (4%)]\tLoss: 1.550980\n",
      "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 1.518378\n",
      "Train Epoch: 5 [7500/60000 (12%)]\tLoss: 1.571352\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 1.613103\n",
      "Train Epoch: 5 [12500/60000 (21%)]\tLoss: 1.574910\n",
      "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 1.560844\n",
      "Train Epoch: 5 [17500/60000 (29%)]\tLoss: 1.565130\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 1.546188\n",
      "Train Epoch: 5 [22500/60000 (38%)]\tLoss: 1.563267\n",
      "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 1.553696\n",
      "Train Epoch: 5 [27500/60000 (46%)]\tLoss: 1.570960\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 1.547704\n",
      "Train Epoch: 5 [32500/60000 (54%)]\tLoss: 1.523524\n",
      "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 1.504814\n",
      "Train Epoch: 5 [37500/60000 (62%)]\tLoss: 1.563036\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.531314\n",
      "Train Epoch: 5 [42500/60000 (71%)]\tLoss: 1.523873\n",
      "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 1.548159\n",
      "Train Epoch: 5 [47500/60000 (79%)]\tLoss: 1.554552\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.550396\n",
      "Train Epoch: 5 [52500/60000 (88%)]\tLoss: 1.572315\n",
      "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 1.533154\n",
      "Train Epoch: 5 [57500/60000 (96%)]\tLoss: 1.563654\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9673/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.542292\n",
      "Train Epoch: 6 [2500/60000 (4%)]\tLoss: 1.513023\n",
      "Train Epoch: 6 [5000/60000 (8%)]\tLoss: 1.529552\n",
      "Train Epoch: 6 [7500/60000 (12%)]\tLoss: 1.557100\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 1.599439\n",
      "Train Epoch: 6 [12500/60000 (21%)]\tLoss: 1.518482\n",
      "Train Epoch: 6 [15000/60000 (25%)]\tLoss: 1.520881\n",
      "Train Epoch: 6 [17500/60000 (29%)]\tLoss: 1.557927\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.564229\n",
      "Train Epoch: 6 [22500/60000 (38%)]\tLoss: 1.514269\n",
      "Train Epoch: 6 [25000/60000 (42%)]\tLoss: 1.514675\n",
      "Train Epoch: 6 [27500/60000 (46%)]\tLoss: 1.490114\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 1.534321\n",
      "Train Epoch: 6 [32500/60000 (54%)]\tLoss: 1.552629\n",
      "Train Epoch: 6 [35000/60000 (58%)]\tLoss: 1.528555\n",
      "Train Epoch: 6 [37500/60000 (62%)]\tLoss: 1.512800\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 1.539369\n",
      "Train Epoch: 6 [42500/60000 (71%)]\tLoss: 1.535632\n",
      "Train Epoch: 6 [45000/60000 (75%)]\tLoss: 1.522912\n",
      "Train Epoch: 6 [47500/60000 (79%)]\tLoss: 1.535867\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 1.552974\n",
      "Train Epoch: 6 [52500/60000 (88%)]\tLoss: 1.496309\n",
      "Train Epoch: 6 [55000/60000 (92%)]\tLoss: 1.533381\n",
      "Train Epoch: 6 [57500/60000 (96%)]\tLoss: 1.525401\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.541042\n",
      "Train Epoch: 7 [2500/60000 (4%)]\tLoss: 1.505110\n",
      "Train Epoch: 7 [5000/60000 (8%)]\tLoss: 1.522357\n",
      "Train Epoch: 7 [7500/60000 (12%)]\tLoss: 1.513358\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 1.504004\n",
      "Train Epoch: 7 [12500/60000 (21%)]\tLoss: 1.530787\n",
      "Train Epoch: 7 [15000/60000 (25%)]\tLoss: 1.557254\n",
      "Train Epoch: 7 [17500/60000 (29%)]\tLoss: 1.553996\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 1.559363\n",
      "Train Epoch: 7 [22500/60000 (38%)]\tLoss: 1.522437\n",
      "Train Epoch: 7 [25000/60000 (42%)]\tLoss: 1.505853\n",
      "Train Epoch: 7 [27500/60000 (46%)]\tLoss: 1.511911\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 1.522758\n",
      "Train Epoch: 7 [32500/60000 (54%)]\tLoss: 1.525354\n",
      "Train Epoch: 7 [35000/60000 (58%)]\tLoss: 1.561551\n",
      "Train Epoch: 7 [37500/60000 (62%)]\tLoss: 1.530551\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 1.567796\n",
      "Train Epoch: 7 [42500/60000 (71%)]\tLoss: 1.477656\n",
      "Train Epoch: 7 [45000/60000 (75%)]\tLoss: 1.548872\n",
      "Train Epoch: 7 [47500/60000 (79%)]\tLoss: 1.510079\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.515047\n",
      "Train Epoch: 7 [52500/60000 (88%)]\tLoss: 1.539612\n",
      "Train Epoch: 7 [55000/60000 (92%)]\tLoss: 1.549177\n",
      "Train Epoch: 7 [57500/60000 (96%)]\tLoss: 1.521185\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9708/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.513140\n",
      "Train Epoch: 8 [2500/60000 (4%)]\tLoss: 1.497941\n",
      "Train Epoch: 8 [5000/60000 (8%)]\tLoss: 1.534283\n",
      "Train Epoch: 8 [7500/60000 (12%)]\tLoss: 1.531511\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 1.544151\n",
      "Train Epoch: 8 [12500/60000 (21%)]\tLoss: 1.478134\n",
      "Train Epoch: 8 [15000/60000 (25%)]\tLoss: 1.516276\n",
      "Train Epoch: 8 [17500/60000 (29%)]\tLoss: 1.544442\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 1.540064\n",
      "Train Epoch: 8 [22500/60000 (38%)]\tLoss: 1.517885\n",
      "Train Epoch: 8 [25000/60000 (42%)]\tLoss: 1.530628\n",
      "Train Epoch: 8 [27500/60000 (46%)]\tLoss: 1.519271\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 1.551750\n",
      "Train Epoch: 8 [32500/60000 (54%)]\tLoss: 1.568921\n",
      "Train Epoch: 8 [35000/60000 (58%)]\tLoss: 1.540193\n",
      "Train Epoch: 8 [37500/60000 (62%)]\tLoss: 1.508673\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 1.544013\n",
      "Train Epoch: 8 [42500/60000 (71%)]\tLoss: 1.504723\n",
      "Train Epoch: 8 [45000/60000 (75%)]\tLoss: 1.518149\n",
      "Train Epoch: 8 [47500/60000 (79%)]\tLoss: 1.526767\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.492973\n",
      "Train Epoch: 8 [52500/60000 (88%)]\tLoss: 1.558999\n",
      "Train Epoch: 8 [55000/60000 (92%)]\tLoss: 1.541932\n",
      "Train Epoch: 8 [57500/60000 (96%)]\tLoss: 1.529742\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.487783\n",
      "Train Epoch: 9 [2500/60000 (4%)]\tLoss: 1.519657\n",
      "Train Epoch: 9 [5000/60000 (8%)]\tLoss: 1.504301\n",
      "Train Epoch: 9 [7500/60000 (12%)]\tLoss: 1.489629\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 1.521951\n",
      "Train Epoch: 9 [12500/60000 (21%)]\tLoss: 1.546066\n",
      "Train Epoch: 9 [15000/60000 (25%)]\tLoss: 1.548037\n",
      "Train Epoch: 9 [17500/60000 (29%)]\tLoss: 1.540609\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 1.536387\n",
      "Train Epoch: 9 [22500/60000 (38%)]\tLoss: 1.527009\n",
      "Train Epoch: 9 [25000/60000 (42%)]\tLoss: 1.532378\n",
      "Train Epoch: 9 [27500/60000 (46%)]\tLoss: 1.513046\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 1.528312\n",
      "Train Epoch: 9 [32500/60000 (54%)]\tLoss: 1.547372\n",
      "Train Epoch: 9 [35000/60000 (58%)]\tLoss: 1.537017\n",
      "Train Epoch: 9 [37500/60000 (62%)]\tLoss: 1.554393\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 1.547344\n",
      "Train Epoch: 9 [42500/60000 (71%)]\tLoss: 1.485612\n",
      "Train Epoch: 9 [45000/60000 (75%)]\tLoss: 1.507617\n",
      "Train Epoch: 9 [47500/60000 (79%)]\tLoss: 1.540328\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 1.517937\n",
      "Train Epoch: 9 [52500/60000 (88%)]\tLoss: 1.497148\n",
      "Train Epoch: 9 [55000/60000 (92%)]\tLoss: 1.493716\n",
      "Train Epoch: 9 [57500/60000 (96%)]\tLoss: 1.526525\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9747/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.503760\n",
      "Train Epoch: 10 [2500/60000 (4%)]\tLoss: 1.518152\n",
      "Train Epoch: 10 [5000/60000 (8%)]\tLoss: 1.518362\n",
      "Train Epoch: 10 [7500/60000 (12%)]\tLoss: 1.534965\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 1.536968\n",
      "Train Epoch: 10 [12500/60000 (21%)]\tLoss: 1.503311\n",
      "Train Epoch: 10 [15000/60000 (25%)]\tLoss: 1.515928\n",
      "Train Epoch: 10 [17500/60000 (29%)]\tLoss: 1.550422\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 1.530279\n",
      "Train Epoch: 10 [22500/60000 (38%)]\tLoss: 1.511641\n",
      "Train Epoch: 10 [25000/60000 (42%)]\tLoss: 1.532293\n",
      "Train Epoch: 10 [27500/60000 (46%)]\tLoss: 1.564311\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 1.526096\n",
      "Train Epoch: 10 [32500/60000 (54%)]\tLoss: 1.523789\n",
      "Train Epoch: 10 [35000/60000 (58%)]\tLoss: 1.502966\n",
      "Train Epoch: 10 [37500/60000 (62%)]\tLoss: 1.556174\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 1.548137\n",
      "Train Epoch: 10 [42500/60000 (71%)]\tLoss: 1.536392\n",
      "Train Epoch: 10 [45000/60000 (75%)]\tLoss: 1.521181\n",
      "Train Epoch: 10 [47500/60000 (79%)]\tLoss: 1.497533\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 1.502480\n",
      "Train Epoch: 10 [52500/60000 (88%)]\tLoss: 1.517275\n",
      "Train Epoch: 10 [55000/60000 (92%)]\tLoss: 1.534340\n",
      "Train Epoch: 10 [57500/60000 (96%)]\tLoss: 1.553170\n",
      "\n",
      "Test set: Average loss: 0.0149, Accuracy: 9747/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating functions for training and testing\n",
    "\n",
    "model = CNN().to(device)  # Creating the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Creating the optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # Creating the loss function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "        data,target = data.to(device),target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 25 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(loaders['train'].dataset),\n",
    "                100. * batch_idx / len(loaders['train']), loss.item()))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(loaders['test'].dataset),\n",
    "        100. * correct / len(loaders['test'].dataset)))\n",
    "    \n",
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roboc\\AppData\\Local\\Temp\\ipykernel_6148\\4231561469.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predictions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "data,target = test_dataset[4]\n",
    "data = data.unsqueeze(0).to(device)\n",
    "output = model(data)\n",
    "predictions = output.argmax(dim=1, keepdim=True).item()\n",
    "print(\"predictions\",predictions)\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
